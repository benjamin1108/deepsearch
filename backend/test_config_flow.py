#!/usr/bin/env python3
"""æµ‹è¯•é…ç½®å‚æ•°ä¼ é€’æµç¨‹"""

from dotenv import load_dotenv
load_dotenv()

from agent.configuration import Configuration
from agent.llm_factory import LLMFactory

def test_config_flow():
    print("=" * 60)
    print("æµ‹è¯•é…ç½®å‚æ•°ä¼ é€’æµç¨‹")
    print("=" * 60)
    
    # æ¨¡æ‹Ÿå‰ç«¯ä¼ é€’çš„é…ç½®ï¼ˆç±»ä¼¼App.tsxä¸­çš„å‚æ•°ï¼‰
    mock_config = {
        "llm_provider": "qwen",
        "query_generator_model": "qwen-plus",
        "reflection_model": "qwen-max",
        "answer_model": "qwen-max",
        "initial_search_query_count": 3,
        "max_research_loops": 3
    }
    
    print("\nğŸ”§ æ¨¡æ‹Ÿå‰ç«¯ä¼ é€’çš„é…ç½®:")
    for key, value in mock_config.items():
        print(f"  {key}: {value}")
    
    # æµ‹è¯•Configuration.from_runnable_config
    print("\nğŸ“‹ æµ‹è¯•Configuration.from_runnable_config:")
    try:
        config = Configuration.from_runnable_config(mock_config)
        print(f"  âœ… åˆ›å»ºConfigurationæˆåŠŸ")
        print(f"  Provider: {config.llm_provider}")
        print(f"  Query Model: {config.query_generator_model}")
        print(f"  Reflection Model: {config.reflection_model}")
        print(f"  Answer Model: {config.answer_model}")
    except Exception as e:
        print(f"  âŒ åˆ›å»ºConfigurationå¤±è´¥: {e}")
        return
    
    # æµ‹è¯•LLMåˆ›å»º
    print("\nğŸ¤– æµ‹è¯•LLMåˆ›å»º:")
    try:
        llm = LLMFactory.create_llm(
            provider=config.llm_provider,
            model_name=config.query_generator_model,
            temperature=0.0,
            max_retries=2
        )
        print(f"  âœ… LLMåˆ›å»ºæˆåŠŸ: {type(llm).__name__}")
        print(f"  Provider: {config.llm_provider}")
        print(f"  Model: {config.query_generator_model}")
    except Exception as e:
        print(f"  âŒ LLMåˆ›å»ºå¤±è´¥: {e}")
    
    # æµ‹è¯•é»˜è®¤Geminié…ç½®ï¼ˆé—®é¢˜åœºæ™¯ï¼‰
    print("\nâš ï¸  æµ‹è¯•é»˜è®¤Geminié…ç½®ï¼ˆé—®é¢˜åœºæ™¯ï¼‰:")
    try:
        default_config = Configuration()  # é»˜è®¤é…ç½®
        print(f"  é»˜è®¤Provider: {default_config.llm_provider}")
        print(f"  é»˜è®¤Query Model: {default_config.query_generator_model}")
        
        # å°è¯•åˆ›å»ºGemini LLMï¼ˆè¿™é‡Œåº”è¯¥ä¼šå¤±è´¥ï¼Œå› ä¸ºä½ç½®é™åˆ¶ï¼‰
        gemini_llm = LLMFactory.create_llm(
            provider=default_config.llm_provider,
            model_name=default_config.query_generator_model,
            temperature=0.0,
            max_retries=2
        )
        print(f"  âœ… Gemini LLMåˆ›å»ºæˆåŠŸ: {type(gemini_llm).__name__}")
    except Exception as e:
        print(f"  âŒ Gemini LLMåˆ›å»ºå¤±è´¥: {e}")
        print(f"     è¿™è¯´æ˜é—®é¢˜å¯èƒ½å‡ºç°åœ¨è¿™é‡Œï¼")

if __name__ == "__main__":
    test_config_flow() 