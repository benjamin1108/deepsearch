#!/usr/bin/env python3
"""æµ‹è¯•qwenæ¨¡å‹çš„ç»“æ„åŒ–è¾“å‡ºåŠŸèƒ½"""

from dotenv import load_dotenv
load_dotenv()

from agent.llm_factory import LLMFactory
from agent.tools_and_schemas import SearchQueryList, Reflection

def test_qwen_structured_output():
    print("=" * 60)
    print("æµ‹è¯•Qwenæ¨¡å‹çš„ç»“æ„åŒ–è¾“å‡ºåŠŸèƒ½")
    print("=" * 60)
    
    try:
        # åˆ›å»ºqwen LLM
        llm = LLMFactory.create_llm(
            provider="qwen",
            model_name="qwen-plus",
            temperature=0.5,
            max_retries=2
        )
        print("âœ… Qwen LLMåˆ›å»ºæˆåŠŸ")
        
        # æµ‹è¯•ç®€å•çš„æ–‡æœ¬è¾“å‡º
        print("\nğŸ“ æµ‹è¯•ç®€å•æ–‡æœ¬è¾“å‡º:")
        simple_prompt = "è¯·ç”¨ä¸€å¥è¯è§£é‡Šä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½"
        simple_result = llm.invoke(simple_prompt)
        print(f"  ç»“æœ: {simple_result.content[:100]}...")
        
        # æµ‹è¯•SearchQueryListç»“æ„åŒ–è¾“å‡º
        print("\nğŸ” æµ‹è¯•SearchQueryListç»“æ„åŒ–è¾“å‡º:")
        structured_llm = llm.with_structured_output(SearchQueryList)
        
        search_prompt = """ä¸ºäº†ç ”ç©¶"äººå·¥æ™ºèƒ½çš„å‘å±•å†å²"è¿™ä¸ªä¸»é¢˜ï¼Œè¯·ç”Ÿæˆ2ä¸ªç›¸å…³çš„æœç´¢æŸ¥è¯¢ã€‚

è¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¿”å›JSON:
{
    "query": ["æŸ¥è¯¢1", "æŸ¥è¯¢2"],
    "rationale": "ä¸ºä»€ä¹ˆé€‰æ‹©è¿™äº›æŸ¥è¯¢çš„åŸå› "
}
"""
        
        try:
            search_result = structured_llm.invoke(search_prompt)
            if search_result is None:
                print("  âŒ ç»“æ„åŒ–è¾“å‡ºè¿”å›None")
            else:
                print(f"  âœ… ç»“æ„åŒ–è¾“å‡ºæˆåŠŸ")
                print(f"  æŸ¥è¯¢åˆ—è¡¨: {search_result.query}")
                print(f"  ç†ç”±: {search_result.rationale}")
        except Exception as e:
            print(f"  âŒ ç»“æ„åŒ–è¾“å‡ºå¤±è´¥: {e}")
            print(f"  é”™è¯¯ç±»å‹: {type(e).__name__}")
        
        # æµ‹è¯•Reflectionç»“æ„åŒ–è¾“å‡º
        print("\nğŸ¤” æµ‹è¯•Reflectionç»“æ„åŒ–è¾“å‡º:")
        reflection_llm = llm.with_structured_output(Reflection)
        
        reflection_prompt = """åŸºäºä»¥ä¸‹æ€»ç»“ï¼Œåˆ¤æ–­æ˜¯å¦éœ€è¦æ›´å¤šä¿¡æ¯ï¼š

æ€»ç»“ï¼šäººå·¥æ™ºèƒ½æ˜¯ä¸€ç§è®¡ç®—æœºç§‘å­¦æŠ€æœ¯ã€‚

è¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¿”å›JSON:
{
    "is_sufficient": true/false,
    "knowledge_gap": "ç¼ºå°‘çš„ä¿¡æ¯æè¿°",
    "follow_up_queries": ["åç»­æŸ¥è¯¢1", "åç»­æŸ¥è¯¢2"]
}
"""
        
        try:
            reflection_result = reflection_llm.invoke(reflection_prompt)
            if reflection_result is None:
                print("  âŒ Reflectionç»“æ„åŒ–è¾“å‡ºè¿”å›None")
            else:
                print(f"  âœ… Reflectionç»“æ„åŒ–è¾“å‡ºæˆåŠŸ")
                print(f"  æ˜¯å¦è¶³å¤Ÿ: {reflection_result.is_sufficient}")
                print(f"  çŸ¥è¯†ç¼ºå£: {reflection_result.knowledge_gap}")
                print(f"  åç»­æŸ¥è¯¢: {reflection_result.follow_up_queries}")
        except Exception as e:
            print(f"  âŒ Reflectionç»“æ„åŒ–è¾“å‡ºå¤±è´¥: {e}")
            print(f"  é”™è¯¯ç±»å‹: {type(e).__name__}")
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    test_qwen_structured_output() 